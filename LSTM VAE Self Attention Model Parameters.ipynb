{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":14348.48869,"end_time":"2022-10-26T14:03:33.948105","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-10-26T10:04:25.459415","version":"2.3.4"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# Training parameters\n\nwindow = 672       # 1 week\nstride = 4         # 1 hour\nlatent_dim = 10    # Autoencoder latent dimension\nepochs = 150       # Number of epochs\nbatch_size = 8     # Batch size\nM = 200            # Montecarlo","metadata":{"_cell_guid":"ee9dc1b8-8938-45b1-a268-fd5dbcfd54dc","_uuid":"6eec9bc4-88cc-4d5a-b660-23b78027559a","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022987,"end_time":"2022-10-26T10:04:42.510591","exception":false,"start_time":"2022-10-26T10:04:42.487604","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SimScore(tfk.layers.Layer):\n\n    def call(self, inputs):\n        seq, h_dim = inputs  # seq: batch x x_dim x h_dim*2\n\n        S = tf.map_fn(fn=lambda t: tf.linalg.matmul(tf.transpose(t), t), elems=seq) # batch x h_dim x h_dim\n        S = tf.map_fn(fn=lambda t: t / tf.math.sqrt((tf.cast(h_dim*2, dtype=tf.float32))), elems=S)\n        A = tf.map_fn(fn=lambda t: tf.keras.activations.softmax(t), elems=S)\n        C = tf.matmul(seq, A)\n\n        return C","metadata":{"_cell_guid":"c7fc4192-cc4a-4c0f-9224-442633b12811","_uuid":"0ff6e0e9-3718-4233-8be7-1a0e061fc0f8","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.025498,"end_time":"2022-10-26T10:04:43.200090","exception":false,"start_time":"2022-10-26T10:04:43.174592","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AddNoise(tfk.layers.Layer):\n    \n    def call(self, inputs):\n        data, noise_factor = inputs\n        noise = tf.map_fn(fn=lambda t: tf.random.normal((672,2), 0, 1)*noise_factor, elems=data)\n        noise_input = data + noise\n        \n        return noise_input","metadata":{"_cell_guid":"98b731e6-2167-448e-b7db-329afdee130e","_uuid":"9cf19f28-650b-4f3f-97e6-f1f5fd03a1c7","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.023645,"end_time":"2022-10-26T10:04:43.239491","exception":false,"start_time":"2022-10-26T10:04:43.215846","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the model\n\ninput_shape = X_train.shape[1:]\noutput_shape = X_train.shape[1:]\n\nfrom keras import backend as K\nfrom tensorflow.keras import Input\n\n###########\n# ENCODER #\n###########\n\nencoder_input = tf.keras.Input(shape=input_shape)\n\nh_seq, forward_h, forward_c = tfkl.LSTM(64, return_sequences=True, return_state=True)(encoder_input)\n\nCdet = SimScore()([h_seq, window])\n\nc_mean = tfkl.Dense(window, activation='linear', name=\"c_mean\")(Cdet)\nc_log_var = tfkl.Dense(window, activation='softplus', name=\"c_var\")(Cdet)\n\n# Latent representation: mean + log of std.dev.\nz_mean = tfkl.Dense(latent_dim, activation='linear', name=\"z_mean\")(forward_h)\nz_log_var = tfkl.Dense(latent_dim, activation='linear', name=\"z_var\")(forward_h)\n\n# Reparametrization trick\ndef sample_z1(args):\n    z_mean, z_log_var = args\n    eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1]))\n    return z_mean + tf.exp(alpha * z_log_var) * eps\n\n# Reparametrization trick\ndef sample_z2(args):\n    z_mean, z_log_var = args\n    eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1], K.int_shape(z_mean)[2]))\n    return z_mean + tf.exp(alpha * z_log_var) * eps\n    \n# Sampling a vector from the latent distribution\nz = tfkl.Lambda(sample_z1, name='z')([z_mean, z_log_var])\nc = tfkl.Lambda(sample_z2, name='c')([c_mean, c_log_var])\n\nencoder = tfk.Model(encoder_input, [z_mean, z_log_var, z, c_mean, c_log_var, c], name='encoder')\nprint(encoder.summary())","metadata":{"papermill":{"duration":3.287207,"end_time":"2022-10-26T10:04:46.542330","exception":false,"start_time":"2022-10-26T10:04:43.255123","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###########\n# DECODER #\n###########\n\nz_inputs = Input(shape=(latent_dim, ), name='decoder_input_1')\nc_inputs = Input(shape=(window, window), name='decoder_input_2')\n\nrepeated = tf.transpose(tfkl.RepeatVector(window)(z_inputs), perm = [0, 2, 1])\nconcat = tf.transpose( tfkl.Concatenate(axis=1)([repeated, c_inputs]), perm = [0, 2, 1] )\n\nx = tfkl.LSTM(64, return_sequences=True)(concat)\nx = tfkl.TimeDistributed(tfkl.Dense(output_shape[1]))(x)\n\nmu = tfkl.Conv1D(X_train.shape[2],2,1, padding='same', name='mu')(x)\nsigma = tfkl.Conv1D(X_train.shape[2],2,1, padding='same', name='sigma')(x)\n\n\n# REPRESENTATION USED ONLY FOR GRAPHICAL PURPOSES\n\n# Reparametrization trick\ndef sample_z2(args):\n    z_mean, z_log_var = args\n    eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1], K.int_shape(z_mean)[2]))\n    return z_mean + tf.exp(alpha * z_log_var) * eps\n\ndecoder_output = tfkl.Lambda(sample_z2, output_shape=(672, 2), name='decoder_output')([mu, sigma])\n\n# Define and summarize decoder model\ndecoder = tfk.Model([z_inputs, c_inputs], [mu, sigma, decoder_output], name='decoder')\ndecoder.summary()","metadata":{"_cell_guid":"d26e93e1-6019-473d-a3e4-757bab505a46","_uuid":"b11ee25e-c328-4870-ac28-8b14a85b217f","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.277946,"end_time":"2022-10-26T10:04:46.836621","exception":false,"start_time":"2022-10-26T10:04:46.558675","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VAE(tfk.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = tfk.metrics.Mean(name=\"total_loss\")\n        self.likelihood_tracker = tfk.metrics.Mean(name=\"likelihood\")\n        self.kl_loss_z_tracker = tfk.metrics.Mean(name=\"kl_loss_z\")\n        self.kl_loss_c_tracker = tfk.metrics.Mean(name=\"kl_loss_c\")\n        self.reconstruction_loss_tracker = tfk.metrics.Mean(name=\"reconstruction_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.likelihood_tracker,\n            self.kl_loss_z_tracker,\n            self.kl_loss_c_tracker,\n            self.reconstruction_loss_tracker\n        ]\n    \n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            \n            # Reparametrization trick\n            def sample_z2(args):\n                z_mean, z_log_var = args\n                eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1], K.int_shape(z_mean)[2]))\n                return z_mean + tf.exp(alpha * z_log_var) * eps\n            \n            encoder_mu, encoder_log_var, z, c_mean, c_log_var, c = self.encoder(data)\n            decoder_mu, decoder_log_var, _ = self.decoder([z,c])\n            decoder_sigma = tf.exp(decoder_log_var)\n                             \n            pdf_normal = tfp.distributions.MultivariateNormalDiag(decoder_mu, decoder_sigma, validate_args=True, name='Gauss')\n            likelihood = -(pdf_normal.log_prob(data))\n            likelihood = tf.reduce_mean(likelihood, axis=-1)\n            likelihood = tf.reduce_mean(likelihood, axis=-1)\n                \n            decoder_output = tfkl.Lambda(sample_z2, output_shape=input_shape, name='decoder_output')([decoder_mu, decoder_log_var])\n            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, decoder_output), axis=1))\n            \n            kl_loss_z = -0.5 * (1 + encoder_log_var - tf.square(encoder_mu) - tf.exp(encoder_log_var))\n            kl_loss_z = tf.reduce_mean(tf.reduce_sum(kl_loss_z, axis=1))\n            \n            kl_loss_c = -0.5 * (1 + c_log_var - tf.square(c_mean) - tf.exp(c_log_var))\n            kl_loss_c = tf.keras.backend.sum(kl_loss_c, axis=1)\n            kl_loss_c = tf.reduce_mean(kl_loss_c, axis=1)\n\n            total_loss = 3*likelihood + kl_loss_z + 0.5*kl_loss_c + reconstruction_loss\n            \n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.likelihood_tracker.update_state(likelihood)\n        self.kl_loss_z_tracker.update_state(kl_loss_z)\n        self.kl_loss_c_tracker.update_state(kl_loss_c)\n        \n        return {\n            \"loss\": self.total_loss_tracker.result(),\n            \"likelihood\": self.likelihood_tracker.result(),\n            \"reconstruction_loss\": self.reconstruction_loss_tracker.result()\n        }\n    \n    \n    \n    def test_step(self, data): # https://github.com/keras-team/keras-io/issues/38\n\n        # Reparametrization trick\n        def sample_z2(args):\n            z_mean, z_log_var = args\n            eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1], K.int_shape(z_mean)[2]))\n            return z_mean + tf.exp(alpha * z_log_var) * eps\n            \n        encoder_mu, encoder_log_var, z, c_mean, c_log_var, c = self.encoder(data)\n        decoder_mu, decoder_log_var, _ = self.decoder([z,c])\n        decoder_sigma = tf.exp(decoder_log_var)\n                             \n        pdf_normal = tfp.distributions.MultivariateNormalDiag(decoder_mu, decoder_sigma, validate_args=True, name='Gauss')\n        likelihood = -(pdf_normal.log_prob(data))\n        likelihood = tf.reduce_mean(likelihood, axis=-1)\n        likelihood = tf.reduce_mean(likelihood, axis=-1)\n                \n        decoder_output = tfkl.Lambda(sample_z2, output_shape=input_shape, name='decoder_output')([decoder_mu, decoder_log_var])\n        reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, decoder_output), axis=1))\n            \n        kl_loss_z = -0.5 * (1 + encoder_log_var - tf.square(encoder_mu) - tf.exp(encoder_log_var))\n        kl_loss_z = tf.reduce_mean(tf.reduce_sum(kl_loss_z, axis=1))\n            \n        kl_loss_c = -0.5 * (1 + c_log_var - tf.square(c_mean) - tf.exp(c_log_var))\n        kl_loss_c = tf.keras.backend.sum(kl_loss_c, axis=1)\n        kl_loss_c = tf.reduce_mean(kl_loss_c, axis=1)\n\n        total_loss = 3*likelihood + kl_loss_z + 0.5*kl_loss_c + reconstruction_loss\n            \n        self.total_loss_tracker.update_state(total_loss)\n        self.likelihood_tracker.update_state(likelihood)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_z_tracker.update_state(kl_loss_z)\n        self.kl_loss_c_tracker.update_state(kl_loss_c)\n        \n        return {\n            \"loss\": self.total_loss_tracker.result(),\n            \"likelihood\": self.likelihood_tracker.result(),\n            \"reconstruction_loss\": self.reconstruction_loss_tracker.result()\n        }","metadata":{"_cell_guid":"d57c2bff-ae5a-4c55-90b8-607a3c3f7256","_uuid":"6dad6fcf-5973-4f2a-9cde-dceec79b78b5","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.042608,"end_time":"2022-10-26T10:04:46.895595","exception":false,"start_time":"2022-10-26T10:04:46.852987","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training\nvae = VAE(encoder, decoder)\nvae.compile(optimizer=tfk.optimizers.Adam())\nvae.fit(x = X_train,\n        validation_data = (X_val, None),\n        epochs=epochs, \n        batch_size=batch_size,\n        callbacks=[\n            tfk.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)]\n       )","metadata":{"_cell_guid":"18773f3d-9d9c-42d2-8144-63471bbddd43","_uuid":"e4065470-c134-43d7-924b-44d32fb767dd","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":12550.713095,"end_time":"2022-10-26T13:33:57.625050","exception":false,"start_time":"2022-10-26T10:04:46.911955","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}